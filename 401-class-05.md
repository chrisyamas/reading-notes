# Implementation: Linked Lists

## Vocabulary List

### Big O: Analysis of Algorithm Efficiency

- **running time**: the amount of time that a function needs to complete (a.k.a. time efficiency or complexity)
- **memory space**: the amount of memory resources that a function uses to store data and instructions (a.k.a. space efficiency or complexity)

#### Input Size

- **input size**: referred to with letter `n`; the size of parameter values that are *read* by the algorithm (not just the number of parameters, but accounting for their sizes); as `n` increases, so does running time and memory space

#### Units of Measurement

- **the three primary measurements of running time**: (1) milliseconds, (2) operations, and (3) "Basic Operations"
- **milliseconds**: the time-based unit of measurement for running time (won't be considered for purposes of Big O because it varies based on power of the particular machine running the code)
- **operations**: the code-line-based unit of measurement for running time; the number of lines of code that are executed from start to finish of a function
- **Basic Operation**: unit that comprimises the greatest share of total running time; typically it is the most time consuming operation within the inner-most loop
- **the four sources of memory usage**: the amount of space needed to hold (1) code for the algorithm, (2) input, (3) output, and (4) working space
- **code for the algorithm**: the number of bytes required to store the characters for the instructions specified in the function
- **input**: the data values passed into the function (if function doesn't consider direct input data values, refer to this as Additional Memory Space)
- **output**: the data values returned by the function
- **working space**: the creation of variables and reference points as the function performs calculations (includes Stack Space of recursive function calls)
- **stack space**: how memory usage scales relative to the size of the input

#### Orders of Growth

- **order of growth**: the increase in running time or memory space as the value of `n` grows
- **constant complexity**: O(1) or constant efficiency; refers to when algorithms use the same amount of time or space no matter the inputs (constant value is represented by `1` by rounding down since actual units will likely be greater than `1` but complexity is independent of `n`)
- **logarithmic complexity**: O(lgn); refers to when algorithms see a decrease in rate of complexity growth as value of `n` increases; occurs when performing calculations on sorted data (i.e. as `n` grows, input is chopped in half for each search thus eliminating more input values)
- **linear complexity**: O(n); refers to when memory space and running time required for algorithm is directly determined by size of `n`; very common, often with loops or recursive functions
- **linearithmic complexity**: O(n*lgn); refers to when complexity increases slightly faster than a linear order of growth; describes growth rate of `n` by `lgn`, because complexity grows by both; occurs with "divide and conquer" algorithms like Merge Sort
- **quadratic complexity**: O(n^2); refers to algorithm where complexity growth rate is equal to size of `n` times `n`; occurs with nested loops performing iterative or recursive logic on all values of `n` and immediately iterate/recurse again for each `n` value (ex. brute force comparison functions)
- **cubic complexity**: O(n^3); refers to when algorithmic complexity growth rate occurs even faster than quadratic(by rate of `n` raised to the third degree), typically due to additional set of nested loops
- **exponential complexity**: O(2^n); refers to very rapidly growing complexity due to the *number of iterative or recursive loops* being equal to input size `n`; can occur when examining data sets by comparing against all possible subsets; fibonacci sequence is popular case of this
- **factorial complexity**: O(n!); refers to extremely fast growth rate for space and time requirements relative to input size, such that an extreme amount of calculations are performed for each value within input of size `n`; occurs when calculating all possible permutations of something like a string or array (or perhaps a deck of cards)

#### Worst Case, Best Case, Average Case

- **worst case**: the case described by *Big O(oh)*; the efficiency for the worst possible input of size `n`; runs the longest for all possible inputs of `n`
- **best case**: the case described by *Big Omega*; the efficiency for the best possible input of size `n`; runs the quickest for all possible inputs of `n`
- **average case**: the case described by *Big Theta*; the efficiency for a "typical" or "random" input of size `n`; NOT the best case and worst case averaged, but rather an assumption / best guess about the possible inputs of size `n` and how efficiency might be affected
- **asymptotic notation**: the notation used for Asymptotic analysis, referring to time and space complexity computations; examples are *Big O(oh)*, *Big Omega*, *Big Theta*
- **Big O(oh)**: notation for an algorithm's asymptotic worst case; order of growth represents time and space *upper bounds*
- **Big Omega**: notation for an algorithm's asymptotic best case; order of growth represents time and space *lower bounds*
- **Big Theta**: notation for an algorithm's asymptotic average case; order of growth represents time and space *tight bound* (need idea of what input predicts better/worse performance of function to determine "typical" input, so *tight* means both upper and lower bounds can be set by this bound)

### Linked Lists

#### What is a Linked List: Terminology

- **linked list**: data structure that contains nodes that links/points to the next node in the list
- **singly**: refers to the number of references the node has; a `Singly` linked list means that there is only one reference, and the reference points to the `Next` node in a linked list
- **doubly**: refers to there being two (double) references within the node. A `Doubly` linked list means that there is a reference to both the `Next` and `Previous` node
- **node**: the individual items/links that live in a linked list; each node contains the data for each link
- **next**: each node contains a property called `Next`; this property contains the reference to the next node
- **head**: a reference of type `Node` to the first node in a linked list
- **current**: the `Current` is a reference of type `Node` to the node that is currently being looked at; when traversing, you create a new `Current` variable at the `Head` to guarantee you are starting from the beginning of the linked list

### What's a Linked List, Anyway? [Part 1]

- **data structures**: the different ways that we can organize data (examples are variables, arrays, hashes, objects, etc.)
- **linear data structures**: those possessing a sequence and an order to how they are constructed and traversed
- **non-linear data structures**: those where items don't have to be arranged in order, so the structure can be traverse *non-sequentially*; examples include hashes, trees, graphs, and more
- **hash**: a.k.a. a dictionary; one example of a non-linear data structure
- **static data structure**: needs all of its resources to be allocated when the structure is created; needs a given size and amount of memory (even if it were to shrink in size or elements get added/removed); the size is fixed, unchanging
- **dynamic data structure**: can shrink and grow in memory; does not need set amount of memory allocated to exist; both shape and size can change
- **singly linked lists**: the simplest type, only go in one direction; there is a single track that we can traverse the list in, starting at head node, traversed from root until last node, ending with empty null value
- **doubly linked lists: contains two references within each node (one to the next node, and one to the previous node); helpful for traversing data structures backwards and forwards