# Notes on Web Scraping

## Web Scrape with Python in 4 minutes

- **web scraping** is a technique to automatically access and extract large amounts of information from a website, which can save a huge amount of time and effort
- most site's terms and conditions prohibit data use for commercial purposes
- downloading too rapidly could break website or get you blocked
- use "Inspect" devtools, click top left arrow symbol, then click on area of site itself ot see code for that item in console


### Python code

- import libraries
- set url to the website and response to a requests.get(url)
- parse html with BeautifulSoup()
- use method .findAll()
- can extract links from `<a>` tags with `['href']`
- time.sleep(1) adds pause to code so we don't get flagged as spammer
- the JupyterNotebook for this can be found [here](https://github.com/julia-git/webscraping_ny_mta)

## What is Web Scraping?

Wikipedia defines web scraping as "data scraping used for extracting data from websites. The web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis."

- involves fetching and extracting from web page
- fetching: downloading of a page, like what a browser does for users to view it
- web crawling is main component of web scraping
- once fetch, extraction
- content can be parsed, searched, reformatted, data copied into spreadsheet or loaded into database
- web scrapes take something out of page for another purpose elsewhere (i.e. find and copy names and phone numbers, or companies and their URLs, etc.)

## How to scrape websites without getting blocked

- web scraping must be done responsibly, or harm can be done

### Respect Robots.txt

- don't try to fetch data that is forbidden by Robots.txt (usually found at root directory of website)
- if you do, try not to scrape too fast, don't follow same pattern with each request, and specifying a 'User-Agent' so you don't seem like a popular browser, or user a user agent string of a very old browser

### Make the crawling slower, do not slam the server, treat websites nicely

- make your spider look real, by mimicking human actions
- slow it down with sleep calls, delays of 10-20 seconds
- use auto throttling mechanisms to adjust spider ot optimum crawling speed

### Additional tips

- Do not follow the same crawling pattern
- Make requests through Proxies and rotate them as needed
- Rotate User Agents and corresponding HTTP Request Headers between requests
- Use a headless browser like Puppeteer, Selenium or Playwright
- Beware of Honey Pot Traps
- Check if Website is Changing Layouts
- Avoid scraping data behind a login
- Use Captcha Solving Services
- How can websites detect and block web scraping?
- How do you find out if a website has blocked or banned you?
